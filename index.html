<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <link rel="icon" href="./img/favicon.png">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Gianluca Bencomo's personal website: professional contacts, bio, publications, and social links">
    <meta name="keywords" content="Gianluca Bencomo, machine learning, Princeton, research, computer science">
    <meta name="author" content="Gianluca Bencomo">
    <meta name="google-site-verification" content="ecgGASIgl7Mn41axpEzAQgZvp4zQ08vZvIiHW6OePLw">
    <title>Gianluca Bencomo</title>
    <link rel="stylesheet" href="style.css">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y4C6XFXPF4"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-Y4C6XFXPF4');
    </script>
  </head>
  <body>
    <!-- Header Section -->
    <header>
      <div class="header-container">
        <div class="header-left">
            <h1 class="header-name"><a href="./index.html">Gianluca Bencomo</a></h1>
        </div>
        <div class="header-right">
          <a href="https://x.com/gianlucabencomo" aria-label="Visit X">X</a>
          <a href="https://github.com/gianlucabencomo" aria-label="Visit GitHub">GitHub</a>
          <a href="https://scholar.google.com/citations?user=xSS55BgAAAAJ&hl=en" aria-label="Visit Google Scholar">Google Scholar</a>
          <a href="./blog.html" aria-label="Visit Blog">Blog</a>
        </div>
      </div>
    </header>

    <main>
      <!-- Biography Section -->
      <section id="biography" class="container">
        <h2>Biography</h2>    
        <div class="bio-content">
          <img src="img/gianluca.JPG" alt="Portrait of Gianluca Bencomo" class="bio-img">
          <div class="bio-text">
            <p>
                I am the president and co-founder of <a href="https://luduslabs.ai">Ludus Labs</a> and a PhD student in Computer Science at Princeton University (currently on-leave). I am advised by <a href="https://cocosci.princeton.edu/tom/index.php">Tom Griffiths</a>. My CV == <a href="./CV.pdf">CV</a>.
            </p>
          </div>
        </div>
      </section>

      <!-- Publications Section -->
      <section id="publications" class="container">
        <h2>Publications</h2>

        <div class="pub">
          <img src="img/teasing.jpg" alt="Publication Image" class="pub-img">
          <div>
            <p class="pub-title">
              <a href="http://arxiv.org/abs/2502.20237">Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks.</a>
            </p>
            <p class="pub-venue">Pre-Print.</p>
            <p class="pub-authors">Gianluca Bencomo, Max Gupta, Ioana Marinescu, R. Thomas McCoy, Thomas L. Griffiths</p>
            <button class="abstract-btn" onclick="toggleAbstract(this)">Show Abstract</button>
            <p class="pub-abstract hidden">Artificial neural networks can acquire many aspects of human knowledge from data, making them promising as models of human learning. But what those networks can learn depends upon their inductive biases -- the factors other than the data that influence the solutions they discover -- and the inductive biases of neural networks remain poorly understood, limiting our ability to draw conclusions about human learning from the performance of these systems. Cognitive scientists and machine learning researchers often focus on the architecture of a neural network as a source of inductive bias. In this paper we explore the impact of another source of inductive bias -- the initial weights of the network -- using meta-learning as a tool for finding initial weights that are adapted for specific problems. We evaluate four widely-used architectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430 different models across three tasks requiring different biases and forms of generalization. We find that meta-learning can substantially reduce or entirely eliminate performance differences across architectures and data representations, suggesting that these factors may be less important as sources of inductive bias than is typically assumed. When differences are present, architectures and data representations that perform well without meta-learning tend to meta-train more effectively. Moreover, all architectures generalize poorly on problems that are far from their meta-training experience, underscoring the need for stronger inductive biases for robust generalization.</p>
          </div>
        </div>


        <div class="pub">
          <img src="img/contrastive.jpg" alt="Publication Image" class="pub-img">
          <div>
            <p class="pub-title">
              <a href="https://arxiv.org/abs/2405.19420">Using Contrastive Learning with Generative Similarity to Learn Spaces that Capture Human Inductive Biases.</a>
            </p>
            <p class="pub-venue">Pre-Print.</p>
            <p class="pub-authors">Raja Marjieh, Sreejan Kumar, Declan Campbell, Liyi Zhang, Gianluca Bencomo, Jake Snell, Thomas Griffiths</p>
            <button class="abstract-btn" onclick="toggleAbstract(this)">Show Abstract</button>
            <p class="pub-abstract hidden">Humans rely on strong inductive biases to learn from few examples and abstract useful information from sensory data. Instilling such biases in machine learning models has been shown to improve their performance on various benchmarks including few-shot learning, robustness, and alignment. However, finding effective training procedures to achieve that goal can be challenging as psychologically-rich training data such as human similarity judgments are expensive to scale, and Bayesian models of human inductive biases are often intractable for complex, realistic domains. Here, we address this challenge by introducing a Bayesian notion of generative similarity whereby two datapoints are considered similar if they are likely to have been sampled from the same distribution. This measure can be applied to complex generative processes, including probabilistic programs. We show that generative similarity can be used to define a contrastive learning objective even when its exact form is intractable, enabling learning of spatial embeddings that express specific inductive biases. We demonstrate the utility of our approach by showing that it can be used to capture human inductive biases for geometric shapes, distinguish different abstract drawing styles that are parameterized by probabilistic programs, and capture abstract high-level categories that enable generalization.</p>
          </div>
        </div>

        <div class="pub">
          <img src="img/circuit.jpg" alt="Publication Image" class="pub-img">
          <div>
            <p class="pub-title">
              <a href="https://arxiv.org/abs/2311.14601">A Metalearned Neural Circuit for Nonparametric Bayesian Inference.</a>
            </p>
            <p class="pub-venue">NeurIPS 2024.</p>
            <p class="pub-authors">Jake Snell, Gianluca Bencomo, Thomas Griffiths</p>
            <button class="abstract-btn" onclick="toggleAbstract(this)">Show Abstract</button>
            <p class="pub-abstract hidden">Most applications of machine learning to classification assume a closed set of balanced classes. This is at odds with the real world, where class occurrence statistics often follow a long-tailed power-law distribution and it is unlikely that all classes are seen in a single sample. Nonparametric Bayesian models naturally capture this phenomenon, but have significant practical barriers to widespread adoption, namely implementation complexity and computational inefficiency. To address this, we present a method for extracting the inductive bias from a nonparametric Bayesian model and transferring it to an artificial neural network. By simulating data with a nonparametric Bayesian prior, we can metalearn a sequence model that performs inference over an unlimited set of classes. After training, this "neural circuit" has distilled the corresponding inductive bias and can successfully perform sequential inference over an open set of classes. Our experimental results show that the metalearned neural circuit achieves comparable or better performance than particle filter-based methods for inference in these models while being faster and simpler to use than methods that explicitly incorporate Bayesian nonparametric inference.</p>
          </div>
        </div>


        <div class="pub">
          <img src="img/implicit.jpg" alt="Publication Image" class="pub-img">
          <div>
            <p class="pub-title">
              <a href="https://arxiv.org/abs/2311.10580">Implicit Maximum a Posteriori Filtering via Adaptive Optimization.</a>
            </p>
            <p class="pub-venue">ICLR 2024.</p>
            <p class="pub-authors">Gianluca Bencomo, Jake Snell, Thomas Griffiths</p>
            <button class="abstract-btn" onclick="toggleAbstract(this)">Show Abstract</button>
            <p class="pub-abstract hidden">Bayesian filtering approximates the true underlying behavior of a time-varying system by inverting an explicit generative model to convert noisy measurements into state estimates. This process typically requires either storage, inversion, and multiplication of large matrices or Monte Carlo estimation, neither of which are practical in high-dimensional state spaces such as the weight spaces of artificial neural networks. Here, we frame the standard Bayesian filtering problem as optimization over a time-varying objective. Instead of maintaining matrices for the filtering equations or simulating particles, we specify an optimizer that defines the Bayesian filter implicitly. In the linear-Gaussian setting, we show that every Kalman filter has an equivalent formulation using K steps of gradient descent. In the nonlinear setting, our experiments demonstrate that our framework results in filters that are effective, robust, and scalable to high-dimensional systems, comparing well against the standard toolbox of Bayesian filtering solutions. We suggest that it is easier to fine-tune an optimizer than it is to specify the correct filtering equations, making our framework an attractive option for high-dimensional filtering problems.</p>
          </div>
        </div>
        

        <div class="pub">
          <img src="img/vendi.jpg" alt="Publication Image" class="pub-img">
          <div>
            <p class="pub-title">
              <a href="https://doi.org/10.1063/5.0166172">Vendi Sampling For Molecular Simulations: Diversity As A Force For Faster Convergence And Better Exploration.</a>
            </p>
            <p class="pub-venue">Journal of Chemical Physics.</p>
            <p class="pub-authors">Amey Pasarkar, Gianluca Bencomo, Simon Olsson, Adji Bousso Dieng</p>
            <button class="abstract-btn" onclick="toggleAbstract(this)">Show Abstract</button>
            <p class="pub-abstract hidden">Molecular dynamics (MD) is the method of choice for understanding the structure, function, and interactions of molecules. However, MD simulations are limited by the strong metastability of many molecules, which traps them in a single conformation basin for an extended amount of time. Enhanced sampling techniques, such as metadynamics and replica exchange, have been developed to overcome this limitation and accelerate the exploration of complex free energy landscapes. In this paper, we propose Vendi Sampling, a replica-based algorithm for increasing the efficiency and efficacy of the exploration of molecular conformation spaces. In Vendi sampling, replicas are simulated in parallel and coupled via a global statistical measure, the Vendi Score, to enhance diversity. Vendi sampling allows for the recovery of unbiased sampling statistics and dramatically improves sampling efficiency. We demonstrate the effectiveness of Vendi sampling in improving molecular dynamics simulations by showing significant improvements in coverage and mixing between metastable states and convergence of free energy estimates for four common benchmarks, including Alanine Dipeptide and Chignolin.</p>
          </div>
        </div>


        <div class="pub">
          <img src="img/brain.jpg" alt="Publication Image" class="pub-img">
          <div>
            <p class="pub-title">
              <a href="https://karger.com/bbe/article-abstract/95/5/272/47302/Illusions-Delusions-and-Your-Backwards-Bayesian?redirectedFrom=fulltext">Illusions, Delusions, and Your Backwards Bayesian Brain: A Biased Visual Perspective.</a>
            </p>
            <p class="pub-venue">Brain, Behavior and Evolution.</p>
            <p class="pub-authors">Richard Born, Gianluca Bencomo</p>
            <button class="abstract-btn" onclick="toggleAbstract(this)">Show Abstract</button>
            <p class="pub-abstract hidden">The retinal image is insufficient for determining what is "out there," because many different real-world geometries could produce any given retinal image. Thus, the visual system must infer which external cause is most likely, given both the sensory data and prior knowledge that is either innate or learned via interactions with the environment. We will describe a general framework of "hierarchical Bayesian inference" that we and others have used to explore the role of cortico-cortical feedback in the visual system, and we will further argue that this approach to "seeing" makes our visual systems prone to perceptual errors in a variety of different ways. In this deliberately provocative and biased perspective, we argue that the neuromodulator, dopamine, may be a crucial link between neural circuits performing Bayesian inference and the perceptual idiosyncrasies of people with schizophrenia.</p>
          </div>
        </div>

      </section>
    </main>

    <footer>
      <p>&copy; 2025 Gianluca Bencomo</p>
    </footer>

    <script>
      function toggleAbstract(button) {
        const abstract = button.nextElementSibling;
        if (abstract.classList.contains("hidden")) {
          abstract.classList.remove("hidden");
          button.textContent = "Hide Abstract";
        } else {
          abstract.classList.add("hidden");
          button.textContent = "Show Abstract";
        }
      }
    </script>
  </body>
</html>

